{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "0zTvbI2n95ju"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import gensim.downloader as api\n",
        "glove_model = api.load(\"glove-wiki-gigaword-300\")\n",
        "embedding_dim = glove_model.vector_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDwZZN9U-CTW",
        "outputId": "35eb17ee-cdcf-4b3d-d751-f6a50ea288ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300\n"
          ]
        }
      ],
      "source": [
        "print(embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efUGFuqh_Mg-",
        "outputId": "0e6f44e0-daa5-4101-b1d4-4a47c236fbec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the first Excel file (news-pal.xlsx)\n",
        "url_pal = \"https://github.com/rosenfa/ai/blob/master/news-pal.xlsx?raw=true\"\n",
        "df_pal = pd.read_excel(url_pal)\n",
        "\n",
        "# Load the second Excel file (news-israel.xlsx)\n",
        "url_israel = \"https://github.com/rosenfa/ai/blob/master/news-israel.xlsx?raw=true\"\n",
        "df_israel = pd.read_excel(url_israel)\n",
        "\n",
        "df_israel.dropna(inplace=True)\n",
        "df_pal.dropna(inplace=True)\n",
        "\n",
        "\n",
        "# Add a 'target' column to both dataframes\n",
        "df_pal['target'] = False  # Assuming 'false' for news-pal.xlsx\n",
        "df_israel['target'] = True   # Assuming 'true' for news-israel.xlsx\n",
        "\n",
        "\n",
        "# Concatenate the two dataframes\n",
        "df_combined = pd.concat([df_pal, df_israel], ignore_index=True)\n",
        "df_combined.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "oRj7jEvNA2Bw"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "# Assuming df_combined is your DataFrame\n",
        "\n",
        "# Define a function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in ENGLISH_STOP_WORDS]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Apply the remove_stopwords function to the 'Content' column\n",
        "df_combined['Content'] = df_combined['Content'].apply(remove_stopwords)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "AtQ5UUlx_YnC"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into features (X) and labels (y)\n",
        "X = df_combined['Content']\n",
        "y = df_combined['target']  # Replace 'target_column' with the actual name of your target column\n",
        "\n",
        "# Split the data into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Now you have your training and test sets ready to be used for further processing or modeling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "LJe5YvDR_8PV"
      },
      "outputs": [],
      "source": [
        "from keras.layers import TextVectorization\n",
        "\n",
        "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(X_train).batch(128)\n",
        "vectorizer.adapt(text_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "91bIaAXawtwI"
      },
      "outputs": [],
      "source": [
        "X_tra = vectorizer(np.array([[s] for s in X_train])).numpy()\n",
        "X_te = vectorizer(np.array([[s] for s in X_test])).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d4tbVtNAu8F",
        "outputId": "84c2ee4d-82f2-44a2-d227-23206b4432b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'gaza',\n",
              " 'israeli',\n",
              " 'israel',\n",
              " 'hamas',\n",
              " 'palestinian',\n",
              " 'war',\n",
              " 'hospital',\n",
              " 'israels']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "vectorizer.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZG9YRdTA-sS",
        "outputId": "43bad916-2069-4d52-e45c-cd2c11b760d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 263,    2,  548, 1176,  263,  177])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "output = vectorizer([[\"the gaza fired on the man\"]])\n",
        "output.numpy()[0, :6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "hPUc3n0uBKnx"
      },
      "outputs": [],
      "source": [
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Hbxwu5-UBd3z"
      },
      "outputs": [],
      "source": [
        "test = [\"gaza\", \"fired\", \"on\", \"israel\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXpKu3csBnwU",
        "outputId": "25077c55-2947-483a-9253-c62a53533e66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 3479 words (756 misses)\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(word_index)\n",
        "num_tokens = len(voc) + 2\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "for word, i in word_index.items():\n",
        "    if word in glove_model:\n",
        "      hits += 1\n",
        "      embedding_matrix[i] = glove_model[word]\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "hPevqgQxB3TU"
      },
      "outputs": [],
      "source": [
        "# print(word_index.items())\n",
        "# print(print(embedding_matrix[10])) #Note that UNK is like OOV from the Lecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "tQYwxHZhB-Zy"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "wqtn8gX2CBpK"
      },
      "outputs": [],
      "source": [
        "from keras import layers\n",
        "\n",
        "def createModel():\n",
        "  int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "  embedded_sequences = embedding_layer(int_sequences_input)\n",
        "  x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
        "  x = layers.MaxPooling1D(5)(x)\n",
        "  x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "  x = layers.MaxPooling1D(5)(x)\n",
        "  x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "  x = layers.GlobalMaxPooling1D()(x)\n",
        "  x = layers.Dense(128, activation=\"relu\")(x)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  preds = layers.Dense(2, activation=\"sigmoid\")(x)\n",
        "  model = keras.Model(int_sequences_input, preds)\n",
        "  model.summary()\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feMuVPxfzPbp",
        "outputId": "8180e31c-191f-462f-ef67-65521e22353a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, None, 300)         1271100   \n",
            "                                                                 \n",
            " conv1d_6 (Conv1D)           (None, None, 128)         192128    \n",
            "                                                                 \n",
            " max_pooling1d_4 (MaxPoolin  (None, None, 128)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_7 (Conv1D)           (None, None, 128)         82048     \n",
            "                                                                 \n",
            " max_pooling1d_5 (MaxPoolin  (None, None, 128)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_8 (Conv1D)           (None, None, 128)         82048     \n",
            "                                                                 \n",
            " global_max_pooling1d_2 (Gl  (None, 128)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1644094 (6.27 MB)\n",
            "Trainable params: 372994 (1.42 MB)\n",
            "Non-trainable params: 1271100 (4.85 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "6/6 [==============================] - 5s 574ms/step - loss: 0.6884 - acc: 0.5250 - val_loss: 0.6604 - val_acc: 0.5053\n",
            "Epoch 2/30\n",
            "6/6 [==============================] - 3s 523ms/step - loss: 0.5946 - acc: 0.7066 - val_loss: 0.5178 - val_acc: 0.8211\n",
            "Epoch 3/30\n",
            "6/6 [==============================] - 4s 758ms/step - loss: 0.4327 - acc: 0.8316 - val_loss: 0.3631 - val_acc: 0.8684\n",
            "Epoch 4/30\n",
            "6/6 [==============================] - 3s 517ms/step - loss: 0.2538 - acc: 0.9053 - val_loss: 0.3103 - val_acc: 0.8737\n",
            "Epoch 5/30\n",
            "6/6 [==============================] - 3s 521ms/step - loss: 0.1155 - acc: 0.9697 - val_loss: 0.2863 - val_acc: 0.8947\n",
            "Epoch 6/30\n",
            "6/6 [==============================] - 3s 522ms/step - loss: 0.0444 - acc: 0.9961 - val_loss: 0.4334 - val_acc: 0.8474\n",
            "Epoch 7/30\n",
            "6/6 [==============================] - 4s 684ms/step - loss: 0.0371 - acc: 0.9855 - val_loss: 0.3813 - val_acc: 0.8895\n",
            "Epoch 8/30\n",
            "6/6 [==============================] - 3s 521ms/step - loss: 0.0193 - acc: 0.9947 - val_loss: 0.5597 - val_acc: 0.8579\n",
            "Epoch 9/30\n",
            "6/6 [==============================] - 4s 721ms/step - loss: 0.0186 - acc: 0.9961 - val_loss: 0.6571 - val_acc: 0.8211\n",
            "Epoch 10/30\n",
            "6/6 [==============================] - 4s 774ms/step - loss: 0.0136 - acc: 0.9934 - val_loss: 0.4967 - val_acc: 0.8842\n",
            "Epoch 11/30\n",
            "6/6 [==============================] - 3s 512ms/step - loss: 0.0070 - acc: 0.9987 - val_loss: 0.5181 - val_acc: 0.8737\n",
            "Epoch 12/30\n",
            "6/6 [==============================] - 3s 518ms/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.4545 - val_acc: 0.9053\n",
            "Epoch 13/30\n",
            "6/6 [==============================] - 3s 526ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.5148 - val_acc: 0.8895\n",
            "Epoch 14/30\n",
            "6/6 [==============================] - 4s 748ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.5471 - val_acc: 0.8895\n",
            "Epoch 15/30\n",
            "6/6 [==============================] - 3s 518ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.5508 - val_acc: 0.8947\n",
            "Epoch 16/30\n",
            "6/6 [==============================] - 3s 515ms/step - loss: 7.0168e-04 - acc: 1.0000 - val_loss: 0.5559 - val_acc: 0.8947\n",
            "Epoch 17/30\n",
            "6/6 [==============================] - 3s 583ms/step - loss: 5.1215e-04 - acc: 1.0000 - val_loss: 0.5629 - val_acc: 0.9000\n",
            "Epoch 18/30\n",
            "6/6 [==============================] - 4s 605ms/step - loss: 4.6920e-04 - acc: 1.0000 - val_loss: 0.5715 - val_acc: 0.9053\n",
            "Epoch 19/30\n",
            "6/6 [==============================] - 3s 508ms/step - loss: 3.4843e-04 - acc: 1.0000 - val_loss: 0.5801 - val_acc: 0.9000\n",
            "Epoch 20/30\n",
            "6/6 [==============================] - 3s 529ms/step - loss: 2.3825e-04 - acc: 1.0000 - val_loss: 0.5900 - val_acc: 0.9000\n",
            "Epoch 21/30\n",
            "6/6 [==============================] - 4s 660ms/step - loss: 2.0261e-04 - acc: 1.0000 - val_loss: 0.6027 - val_acc: 0.9000\n",
            "Epoch 22/30\n",
            "6/6 [==============================] - 3s 514ms/step - loss: 2.0394e-04 - acc: 1.0000 - val_loss: 0.6142 - val_acc: 0.9000\n",
            "Epoch 23/30\n",
            "6/6 [==============================] - 3s 520ms/step - loss: 1.8699e-04 - acc: 1.0000 - val_loss: 0.6262 - val_acc: 0.9000\n",
            "Epoch 24/30\n",
            "6/6 [==============================] - 3s 518ms/step - loss: 9.6039e-05 - acc: 1.0000 - val_loss: 0.6374 - val_acc: 0.9000\n",
            "Epoch 25/30\n",
            "6/6 [==============================] - 4s 780ms/step - loss: 2.3336e-04 - acc: 1.0000 - val_loss: 0.6497 - val_acc: 0.9000\n",
            "Epoch 26/30\n",
            "6/6 [==============================] - 3s 520ms/step - loss: 1.0796e-04 - acc: 1.0000 - val_loss: 0.6620 - val_acc: 0.9000\n",
            "Epoch 27/30\n",
            "6/6 [==============================] - 3s 505ms/step - loss: 1.1468e-04 - acc: 1.0000 - val_loss: 0.6717 - val_acc: 0.9000\n",
            "Epoch 28/30\n",
            "6/6 [==============================] - 3s 518ms/step - loss: 1.8582e-04 - acc: 1.0000 - val_loss: 0.6784 - val_acc: 0.9000\n",
            "Epoch 29/30\n",
            "6/6 [==============================] - 4s 727ms/step - loss: 9.6693e-05 - acc: 1.0000 - val_loss: 0.6813 - val_acc: 0.9000\n",
            "Epoch 30/30\n",
            "6/6 [==============================] - 3s 519ms/step - loss: 7.2499e-05 - acc: 1.0000 - val_loss: 0.6857 - val_acc: 0.9000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78ec1e4c1990>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "model = createModel()\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"],\n",
        ")\n",
        "model.fit(X_tra, y_train, batch_size=128, epochs=30, validation_data=(X_te, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9Hx3qh7HG-e"
      },
      "source": [
        "**Try to explain why this model did as good as it did (really hard to do!!!)  Please add a document and push it separately or add a long text box at the end when you explain what features were in this model and why they did what they to give you your accuracy. **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h9qLDM7HKvl"
      },
      "source": [
        "**The features in this model that help give such a high accuracy was firstly, the transfer learning. We have a model that has already proves succesful. Secondly, we changed the final layer to a sigmoid function for binary classification. Thirdly, we removed teh stopwords which also helped us remove unecessary information which can influence our model negativley.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4zINxkvHrQr"
      },
      "source": [
        "Now use Chi squared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "R1DuRoEfIusA"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Assuming df_combined is your DataFrame with a column 'Content'\n",
        "content_data = df_combined['Content']\n",
        "\n",
        "# Convert text data to a matrix of token counts\n",
        "vz = CountVectorizer()\n",
        "X = vz.fit_transform(content_data)\n",
        "\n",
        "# Apply Chi2 feature selection to get the top 4000 most important words\n",
        "selector = SelectKBest(score_func=chi2, k=4000)\n",
        "X_selected = selector.fit_transform(X, df_combined['target'])  # Assuming you have a target column, change it accordingly\n",
        "\n",
        "# Get the indices of the top 4000 words\n",
        "selected_word_indices = selector.get_support(indices=True)\n",
        "\n",
        "# Get the names of the selected words\n",
        "selected_words = [vz.get_feature_names_out()[i] for i in selected_word_indices]\n",
        "\n",
        "# Function to filter words in a string\n",
        "def filter_content(content):\n",
        "    filtered_words = [word for word in content.split() if word in selected_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Apply the filter_content function to the 'Content' column\n",
        "df_combined['Content'] = df_combined['Content'].apply(filter_content)\n",
        "\n",
        "# Split the data into features (X) and labels (y)\n",
        "X = df_combined['Content']\n",
        "y = df_combined['target']\n",
        "\n",
        "# Split the data into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_tra = vectorizer(np.array([[s] for s in X_train])).numpy()\n",
        "X_te = vectorizer(np.array([[s] for s in X_test])).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = createModel()\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"],\n",
        ")\n",
        "model.fit(X_tra, y_train, batch_size=128, epochs=30, validation_data=(X_te, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzMPo9jzZj8U",
        "outputId": "44b3ce65-c226-4c2d-c13a-aa7675f92f87"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, None, 300)         1271100   \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, None, 128)         192128    \n",
            "                                                                 \n",
            " max_pooling1d_6 (MaxPoolin  (None, None, 128)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_10 (Conv1D)          (None, None, 128)         82048     \n",
            "                                                                 \n",
            " max_pooling1d_7 (MaxPoolin  (None, None, 128)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_11 (Conv1D)          (None, None, 128)         82048     \n",
            "                                                                 \n",
            " global_max_pooling1d_3 (Gl  (None, 128)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1644094 (6.27 MB)\n",
            "Trainable params: 372994 (1.42 MB)\n",
            "Non-trainable params: 1271100 (4.85 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "6/6 [==============================] - 5s 587ms/step - loss: 0.6763 - acc: 0.5789 - val_loss: 0.6378 - val_acc: 0.7053\n",
            "Epoch 2/30\n",
            "6/6 [==============================] - 3s 540ms/step - loss: 0.5629 - acc: 0.7711 - val_loss: 0.4969 - val_acc: 0.7737\n",
            "Epoch 3/30\n",
            "6/6 [==============================] - 4s 778ms/step - loss: 0.3744 - acc: 0.8487 - val_loss: 0.4107 - val_acc: 0.8053\n",
            "Epoch 4/30\n",
            "6/6 [==============================] - 3s 520ms/step - loss: 0.2032 - acc: 0.9316 - val_loss: 0.4240 - val_acc: 0.8211\n",
            "Epoch 5/30\n",
            "6/6 [==============================] - 3s 526ms/step - loss: 0.1139 - acc: 0.9697 - val_loss: 0.4122 - val_acc: 0.8211\n",
            "Epoch 6/30\n",
            "6/6 [==============================] - 3s 532ms/step - loss: 0.0562 - acc: 0.9842 - val_loss: 0.5093 - val_acc: 0.8158\n",
            "Epoch 7/30\n",
            "6/6 [==============================] - 4s 697ms/step - loss: 0.0337 - acc: 0.9908 - val_loss: 0.5123 - val_acc: 0.8000\n",
            "Epoch 8/30\n",
            "6/6 [==============================] - 3s 523ms/step - loss: 0.0138 - acc: 1.0000 - val_loss: 0.5387 - val_acc: 0.8211\n",
            "Epoch 9/30\n",
            "6/6 [==============================] - 3s 533ms/step - loss: 0.0075 - acc: 0.9974 - val_loss: 0.5638 - val_acc: 0.8579\n",
            "Epoch 10/30\n",
            "6/6 [==============================] - 4s 635ms/step - loss: 0.0063 - acc: 0.9974 - val_loss: 0.6254 - val_acc: 0.8579\n",
            "Epoch 11/30\n",
            "6/6 [==============================] - 4s 561ms/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.6563 - val_acc: 0.8632\n",
            "Epoch 12/30\n",
            "6/6 [==============================] - 3s 530ms/step - loss: 0.0049 - acc: 0.9961 - val_loss: 0.6730 - val_acc: 0.8579\n",
            "Epoch 13/30\n",
            "6/6 [==============================] - 3s 527ms/step - loss: 0.0042 - acc: 0.9987 - val_loss: 0.6865 - val_acc: 0.8526\n",
            "Epoch 14/30\n",
            "6/6 [==============================] - 4s 720ms/step - loss: 0.0047 - acc: 0.9961 - val_loss: 0.6990 - val_acc: 0.8421\n",
            "Epoch 15/30\n",
            "6/6 [==============================] - 3s 519ms/step - loss: 0.0043 - acc: 0.9974 - val_loss: 0.7087 - val_acc: 0.8421\n",
            "Epoch 16/30\n",
            "6/6 [==============================] - 4s 694ms/step - loss: 0.0046 - acc: 0.9974 - val_loss: 0.7136 - val_acc: 0.8474\n",
            "Epoch 17/30\n",
            "6/6 [==============================] - 4s 569ms/step - loss: 0.0044 - acc: 0.9961 - val_loss: 0.7181 - val_acc: 0.8474\n",
            "Epoch 18/30\n",
            "6/6 [==============================] - 4s 615ms/step - loss: 0.0039 - acc: 0.9987 - val_loss: 0.7251 - val_acc: 0.8474\n",
            "Epoch 19/30\n",
            "6/6 [==============================] - 3s 522ms/step - loss: 0.0043 - acc: 0.9961 - val_loss: 0.7298 - val_acc: 0.8421\n",
            "Epoch 20/30\n",
            "6/6 [==============================] - 3s 518ms/step - loss: 0.0042 - acc: 0.9961 - val_loss: 0.7347 - val_acc: 0.8474\n",
            "Epoch 21/30\n",
            "6/6 [==============================] - 4s 682ms/step - loss: 0.0039 - acc: 0.9974 - val_loss: 0.7389 - val_acc: 0.8474\n",
            "Epoch 22/30\n",
            "6/6 [==============================] - 3s 525ms/step - loss: 0.0039 - acc: 0.9974 - val_loss: 0.7441 - val_acc: 0.8421\n",
            "Epoch 23/30\n",
            "6/6 [==============================] - 3s 520ms/step - loss: 0.0039 - acc: 0.9974 - val_loss: 0.7495 - val_acc: 0.8421\n",
            "Epoch 24/30\n",
            "6/6 [==============================] - 3s 530ms/step - loss: 0.0044 - acc: 0.9961 - val_loss: 0.7530 - val_acc: 0.8421\n",
            "Epoch 25/30\n",
            "6/6 [==============================] - 4s 729ms/step - loss: 0.0037 - acc: 0.9987 - val_loss: 0.7561 - val_acc: 0.8421\n",
            "Epoch 26/30\n",
            "6/6 [==============================] - 3s 523ms/step - loss: 0.0037 - acc: 0.9974 - val_loss: 0.7616 - val_acc: 0.8526\n",
            "Epoch 27/30\n",
            "6/6 [==============================] - 3s 529ms/step - loss: 0.0037 - acc: 0.9974 - val_loss: 0.7668 - val_acc: 0.8474\n",
            "Epoch 28/30\n",
            "6/6 [==============================] - 3s 599ms/step - loss: 0.0040 - acc: 0.9974 - val_loss: 0.7707 - val_acc: 0.8474\n",
            "Epoch 29/30\n",
            "6/6 [==============================] - 4s 619ms/step - loss: 0.0039 - acc: 0.9974 - val_loss: 0.7747 - val_acc: 0.8474\n",
            "Epoch 30/30\n",
            "6/6 [==============================] - 3s 516ms/step - loss: 0.0035 - acc: 0.9974 - val_loss: 0.7785 - val_acc: 0.8526\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78ec05549720>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}