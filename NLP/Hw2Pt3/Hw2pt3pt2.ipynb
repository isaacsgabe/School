{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zTvbI2n95ju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79e72d58-794a-4bed-ee86-5127b865d681"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 252.1/252.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import gensim.downloader as api\n",
        "glove_model = api.load(\"glove-wiki-gigaword-200\")\n",
        "embedding_dim = glove_model.vector_size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDwZZN9U-CTW",
        "outputId": "3a46b01d-8eff-4443-cbd5-dc05c67fa687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the first Excel file (news-pal.xlsx)\n",
        "url_pal = \"https://github.com/rosenfa/ai/blob/master/news-pal.xlsx?raw=true\"\n",
        "df_pal = pd.read_excel(url_pal)\n",
        "\n",
        "# Load the second Excel file (news-israel.xlsx)\n",
        "url_israel = \"https://github.com/rosenfa/ai/blob/master/news-israel.xlsx?raw=true\"\n",
        "df_israel = pd.read_excel(url_israel)\n",
        "\n",
        "df_israel.dropna(inplace=True)\n",
        "df_pal.dropna(inplace=True)\n",
        "\n",
        "\n",
        "# Add a 'target' column to both dataframes\n",
        "df_pal['target'] = False  # Assuming 'false' for news-pal.xlsx\n",
        "df_israel['target'] = True   # Assuming 'true' for news-israel.xlsx\n",
        "\n",
        "\n",
        "# Concatenate the two dataframes\n",
        "df_combined = pd.concat([df_pal, df_israel], ignore_index=True)\n",
        "df_combined.dropna(inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efUGFuqh_Mg-",
        "outputId": "302aee72-1619-45df-f3f5-90b8fccb1a39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "# Assuming df_combined is your DataFrame\n",
        "\n",
        "# Define a function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in ENGLISH_STOP_WORDS]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Apply the remove_stopwords function to the 'Content' column\n",
        "df_combined['Content'] = df_combined['Content'].apply(remove_stopwords)\n"
      ],
      "metadata": {
        "id": "oRj7jEvNA2Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into features (X) and labels (y)\n",
        "X = df_combined['Content']\n",
        "y = df_combined['target']  # Replace 'target_column' with the actual name of your target column\n",
        "\n",
        "# Split the data into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Now you have your training and test sets ready to be used for further processing or modeling\n"
      ],
      "metadata": {
        "id": "AtQ5UUlx_YnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import TextVectorization\n",
        "\n",
        "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(X_train).batch(128)\n",
        "vectorizer.adapt(text_ds)"
      ],
      "metadata": {
        "id": "LJe5YvDR_8PV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train= vectorizer(np.array([[s] for s in X_train])).numpy()\n",
        "X_test= vectorizer(np.array([[s] for s in X_test])).numpy()"
      ],
      "metadata": {
        "id": "91bIaAXawtwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.get_vocabulary()[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d4tbVtNAu8F",
        "outputId": "e58d5b44-5e2f-4a9f-f712-a1f41a5bf1d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'gaza',\n",
              " 'israeli',\n",
              " 'israel',\n",
              " 'hamas',\n",
              " 'palestinian',\n",
              " 'war',\n",
              " 'hospital',\n",
              " 'israels']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = vectorizer([[\"the gaza fired on the man\"]])\n",
        "output.numpy()[0, :6]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZG9YRdTA-sS",
        "outputId": "12ad82ec-8c46-40f9-9331-bdfa9042ecfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 263,    2,  548, 1176,  263,  177])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ],
      "metadata": {
        "id": "hPUc3n0uBKnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = [\"gaza\", \"fired\", \"on\", \"israel\"]"
      ],
      "metadata": {
        "id": "Hbxwu5-UBd3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word_index)\n",
        "num_tokens = len(voc) + 2\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "for word, i in word_index.items():\n",
        "    if word in glove_model:\n",
        "      hits += 1\n",
        "      embedding_matrix[i] = glove_model[word]\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXpKu3csBnwU",
        "outputId": "13bd07ad-be39-4b24-ed21-9436733d7871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted 3479 words (756 misses)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(word_index.items())\n",
        "# print(print(embedding_matrix[10])) #Note that UNK is like OOV from the Lecture"
      ],
      "metadata": {
        "id": "hPevqgQxB3TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "metadata": {
        "id": "tQYwxHZhB-Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded_sequences = embedding_layer(int_sequences_input)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "preds = layers.Dense(2, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(int_sequences_input, preds)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqtn8gX2CBpK",
        "outputId": "7075dab9-886b-473f-afe4-b600cf489482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, None, 200)         847400    \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, None, 128)         128128    \n",
            "                                                                 \n",
            " max_pooling1d_2 (MaxPoolin  (None, None, 128)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, None, 128)         82048     \n",
            "                                                                 \n",
            " max_pooling1d_3 (MaxPoolin  (None, None, 128)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_5 (Conv1D)           (None, None, 128)         82048     \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Gl  (None, 128)               0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1156394 (4.41 MB)\n",
            "Trainable params: 308994 (1.18 MB)\n",
            "Non-trainable params: 847400 (3.23 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"],\n",
        ")\n",
        "model.fit(X_train, y_train, batch_size=128, epochs=30, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feMuVPxfzPbp",
        "outputId": "8c288793-f016-4b9c-87e7-45392cd103dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "6/6 [==============================] - 5s 687ms/step - loss: 0.6887 - acc: 0.5553 - val_loss: 0.6389 - val_acc: 0.7421\n",
            "Epoch 2/30\n",
            "6/6 [==============================] - 2s 395ms/step - loss: 0.5790 - acc: 0.7447 - val_loss: 0.4728 - val_acc: 0.8474\n",
            "Epoch 3/30\n",
            "6/6 [==============================] - 2s 403ms/step - loss: 0.4117 - acc: 0.8342 - val_loss: 0.3681 - val_acc: 0.8684\n",
            "Epoch 4/30\n",
            "6/6 [==============================] - 2s 379ms/step - loss: 0.2601 - acc: 0.9053 - val_loss: 0.3007 - val_acc: 0.8842\n",
            "Epoch 5/30\n",
            "6/6 [==============================] - 2s 376ms/step - loss: 0.1256 - acc: 0.9605 - val_loss: 0.2987 - val_acc: 0.8632\n",
            "Epoch 6/30\n",
            "6/6 [==============================] - 4s 626ms/step - loss: 0.0688 - acc: 0.9803 - val_loss: 0.3224 - val_acc: 0.8842\n",
            "Epoch 7/30\n",
            "6/6 [==============================] - 2s 399ms/step - loss: 0.0340 - acc: 0.9934 - val_loss: 0.2923 - val_acc: 0.8947\n",
            "Epoch 8/30\n",
            "6/6 [==============================] - 2s 401ms/step - loss: 0.0117 - acc: 1.0000 - val_loss: 0.3110 - val_acc: 0.9000\n",
            "Epoch 9/30\n",
            "6/6 [==============================] - 2s 380ms/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.3852 - val_acc: 0.8947\n",
            "Epoch 10/30\n",
            "6/6 [==============================] - 2s 393ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.3777 - val_acc: 0.8947\n",
            "Epoch 11/30\n",
            "6/6 [==============================] - 3s 619ms/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.4031 - val_acc: 0.8947\n",
            "Epoch 12/30\n",
            "6/6 [==============================] - 2s 376ms/step - loss: 9.0821e-04 - acc: 1.0000 - val_loss: 0.4233 - val_acc: 0.8895\n",
            "Epoch 13/30\n",
            "6/6 [==============================] - 2s 379ms/step - loss: 6.4581e-04 - acc: 1.0000 - val_loss: 0.4487 - val_acc: 0.8895\n",
            "Epoch 14/30\n",
            "6/6 [==============================] - 2s 395ms/step - loss: 7.0803e-04 - acc: 1.0000 - val_loss: 0.4405 - val_acc: 0.8947\n",
            "Epoch 15/30\n",
            "6/6 [==============================] - 2s 404ms/step - loss: 5.7037e-04 - acc: 1.0000 - val_loss: 0.4354 - val_acc: 0.8895\n",
            "Epoch 16/30\n",
            "6/6 [==============================] - 3s 611ms/step - loss: 3.5684e-04 - acc: 1.0000 - val_loss: 0.4377 - val_acc: 0.8947\n",
            "Epoch 17/30\n",
            "6/6 [==============================] - 2s 383ms/step - loss: 2.9590e-04 - acc: 1.0000 - val_loss: 0.4425 - val_acc: 0.8947\n",
            "Epoch 18/30\n",
            "6/6 [==============================] - 2s 400ms/step - loss: 3.6472e-04 - acc: 1.0000 - val_loss: 0.4494 - val_acc: 0.8947\n",
            "Epoch 19/30\n",
            "6/6 [==============================] - 2s 381ms/step - loss: 4.6233e-04 - acc: 1.0000 - val_loss: 0.4559 - val_acc: 0.8842\n",
            "Epoch 20/30\n",
            "6/6 [==============================] - 2s 383ms/step - loss: 3.6769e-04 - acc: 1.0000 - val_loss: 0.4641 - val_acc: 0.8842\n",
            "Epoch 21/30\n",
            "6/6 [==============================] - 3s 553ms/step - loss: 2.2810e-04 - acc: 1.0000 - val_loss: 0.4689 - val_acc: 0.8842\n",
            "Epoch 22/30\n",
            "6/6 [==============================] - 2s 381ms/step - loss: 3.2719e-04 - acc: 1.0000 - val_loss: 0.4722 - val_acc: 0.8895\n",
            "Epoch 23/30\n",
            "6/6 [==============================] - 2s 374ms/step - loss: 2.2814e-04 - acc: 1.0000 - val_loss: 0.4753 - val_acc: 0.8947\n",
            "Epoch 24/30\n",
            "6/6 [==============================] - 2s 395ms/step - loss: 1.5656e-04 - acc: 1.0000 - val_loss: 0.4786 - val_acc: 0.8947\n",
            "Epoch 25/30\n",
            "6/6 [==============================] - 2s 398ms/step - loss: 1.7661e-04 - acc: 1.0000 - val_loss: 0.4828 - val_acc: 0.8947\n",
            "Epoch 26/30\n",
            "6/6 [==============================] - 3s 482ms/step - loss: 2.2489e-04 - acc: 1.0000 - val_loss: 0.4866 - val_acc: 0.8947\n",
            "Epoch 27/30\n",
            "6/6 [==============================] - 3s 440ms/step - loss: 1.7003e-04 - acc: 1.0000 - val_loss: 0.4909 - val_acc: 0.8947\n",
            "Epoch 28/30\n",
            "6/6 [==============================] - 2s 378ms/step - loss: 1.6848e-04 - acc: 1.0000 - val_loss: 0.4947 - val_acc: 0.8947\n",
            "Epoch 29/30\n",
            "6/6 [==============================] - 2s 378ms/step - loss: 1.2543e-04 - acc: 1.0000 - val_loss: 0.4982 - val_acc: 0.8947\n",
            "Epoch 30/30\n",
            "6/6 [==============================] - 2s 370ms/step - loss: 1.4966e-04 - acc: 1.0000 - val_loss: 0.5022 - val_acc: 0.8947\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ee13c1fe980>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}