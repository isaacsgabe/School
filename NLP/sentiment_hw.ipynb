{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isaacsgabe/School/blob/main/NLP/sentiment_hw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aepa9R_Ogorb"
      },
      "source": [
        "Homework #3 is to study Sentiment Analysis with five types of models:\n",
        "\n",
        "1.\tRule-based\n",
        "2.\tBag of Words\n",
        "3.\tShallow embedding with CNN\n",
        "4.  LSTM\n",
        "5.\tTransformer Models\n",
        "\n",
        "I got this idea from my student Itay Etelis and his Huggingface depo is at:\n",
        "https://huggingface.co/pig4431\n",
        "\n",
        "We will discuss all five possibilities in class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEk0XKoihGXz"
      },
      "source": [
        "## YELP Reviews DataSet\n",
        "The **YELP** reviews dataset consists of reviews from Yelp. It is extracted from the Yelp Dataset Challenge 2015 data.\n",
        "\n",
        "The Yelp reviews full star dataset is constructed by Xiang Zhang (xiang.zhang@nyu.edu) from the Yelp Dataset Challenge 2015. It is first used as a text classification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).  Reviews with 1, 2 stars has been marked negative.\n",
        "Reviews with 4, 5 stars has been marked positive.\n",
        "\n",
        "Reviews with 3 stars has been filtered.\n",
        "\n",
        "Full information about this dataset is at:\n",
        "https://www.kaggle.com/datasets/ilhamfp31/yelp-review-dataset\n",
        "\n",
        "You can either download this dataset from there, or use Itay's code with a train/test/validation partition:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jsyQLeTgntQ",
        "outputId": "a25b9e1a-2195-4aab-a310-b62f54f7a7f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets -q\n",
        "\n",
        "import datasets\n",
        "datasets.logging.set_verbosity_error()\n",
        "datasets.disable_progress_bar()\n",
        "\n",
        "from datasets import load_dataset\n",
        "yelp = load_dataset(\"pig4431/yelp_train25k_test5k_valid5k\")\n",
        "yelp_train = yelp['train']\n",
        "yelp_validate = yelp['validate']\n",
        "yelp_test = yelp['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZMywliWi6d1",
        "outputId": "93f20456-83e1-41b3-bf5f-1ba2ef772b8b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'label': 1,\n",
              " 'text': \"Best hot dog i've ever had!  This place was absolutely great.  Such an interesting combination of flavors, but it definitely works.\"}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "yelp_train[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95YTBcfnjEkj"
      },
      "source": [
        "The advantage to having a testing dataset is that you can tune certain parameters and then see if they validate correctly.  This can be done for all 5 models, but is particularly easy for the first two models: Rule-based and Bag-of-Words models.\n",
        "\n",
        "For example, you can use the rule based approach, VADER:\n",
        "\n",
        "https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/\n",
        "by installing:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9UT2J5ujm5w",
        "outputId": "d0cea553-30b0-499e-b3e3-35b72227ed09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.2.2)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install vaderSentiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02C7w6aHj6jk"
      },
      "outputs": [],
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "sid_obj = SentimentIntensityAnalyzer()\n",
        "sentiment_dict = sid_obj.polarity_scores(\"I love programming!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suxTVRj6j7M9",
        "outputId": "51abe3f7-4441-4e1d-b52b-cc38073c021b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'neg': 0.0, 'neu': 0.308, 'pos': 0.692, 'compound': 0.6696}\n"
          ]
        }
      ],
      "source": [
        "print(sentiment_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-17U9iWmWA9"
      },
      "source": [
        "Note that this model yields four different scores: neg, neu, pos, and compound. We'll talk about these four possibilities in class.\n",
        "\n",
        "Note that the compound score is meant to be based of *all* lexicon ratings and is normalized between -1 (most extreme negative) and +1 (most extreme positive).  In the link above they suggest the thresholds for this value:\n",
        "\n",
        "positive sentiment : (compound score >= 0.05)\n",
        "neutral sentiment : (compound score > -0.05) and (compound score < 0.05)\n",
        "negative sentiment : (compound score <= -0.05)\n",
        "\n",
        "However, please check if this is check the actually best threshold for the given dataset but checking a range of values such as:\n",
        "\n",
        "for compound_score in np.arange(-1, 1, 0.1):\n",
        "\n",
        "which can be checked in test dataset and then validated.\n",
        "\n",
        "Please do so!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYrQPXuplwQ_"
      },
      "source": [
        "Please implement the rule-based model here with the hyperparameter tuning for the compound_score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYu9Qcq4l0d0"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(text, threshold):\n",
        "    # Calculate sentiment scores for the text\n",
        "    scores = sid_obj.polarity_scores(text)\n",
        "    compound_score = scores['compound']\n",
        "\n",
        "    # Apply threshold to predict sentiment\n",
        "    if compound_score >= threshold:\n",
        "        return 'positive'\n",
        "    else:\n",
        "        return 'negative'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAlAi1-trk21",
        "outputId": "21c941cf-be3b-4941-9659-2b90e445a303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Threshold: -1.0, Accuracy: 0.5126\n",
            "Threshold: -0.9, Accuracy: 0.559\n",
            "Threshold: -0.8, Accuracy: 0.5944\n",
            "Threshold: -0.7, Accuracy: 0.6192\n",
            "Threshold: -0.6, Accuracy: 0.6388\n",
            "Threshold: -0.5, Accuracy: 0.6594\n",
            "Threshold: -0.4, Accuracy: 0.6748\n",
            "Threshold: -0.3, Accuracy: 0.6852\n",
            "Threshold: -0.2, Accuracy: 0.6964\n",
            "Threshold: -0.1, Accuracy: 0.707\n",
            "Threshold: -0.0, Accuracy: 0.7136\n",
            "Threshold: 0.1, Accuracy: 0.721\n",
            "Threshold: 0.2, Accuracy: 0.7302\n",
            "Threshold: 0.3, Accuracy: 0.7388\n",
            "Threshold: 0.4, Accuracy: 0.7464\n",
            "Threshold: 0.5, Accuracy: 0.7552\n",
            "Threshold: 0.6, Accuracy: 0.7684\n",
            "Threshold: 0.7, Accuracy: 0.768\n",
            "Threshold: 0.8, Accuracy: 0.7692\n",
            "Threshold: 0.9, Accuracy: 0.7438\n",
            "Threshold: 1.0, Accuracy: 0.4874\n",
            "Best threshold: 0.8, Best accuracy: 0.7692\n",
            "Test Accuracy with best threshold: 0.7686\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "best_threshold = None\n",
        "best_accuracy = 0\n",
        "\n",
        "# Define the range of compound score thresholds to test\n",
        "thresholds_to_test = np.arange(-1, 1.1, 0.1)\n",
        "\n",
        "for threshold in thresholds_to_test:\n",
        "    correct_predictions = 0\n",
        "    total_predictions = len(yelp_validate)\n",
        "\n",
        "    for text, label in zip(yelp_validate['text'], yelp_validate['label']):\n",
        "        rounded_threshold = round(threshold, 1)\n",
        "        predicted_sentiment = predict_sentiment(text, rounded_threshold)\n",
        "        pred = 0\n",
        "        if predicted_sentiment == 'positive':\n",
        "          pred = 1\n",
        "        else:\n",
        "          pred = -1\n",
        "        if pred == label:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    print(f\"Threshold: {rounded_threshold}, Accuracy: {accuracy}\")\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_threshold = rounded_threshold\n",
        "\n",
        "print(f\"Best threshold: {best_threshold}, Best accuracy: {best_accuracy}\")\n",
        "\n",
        "# Apply the best threshold on the test set\n",
        "test_correct_predictions = 0\n",
        "test_total_predictions = len(yelp_test)\n",
        "\n",
        "for text, label in zip(yelp_test['text'], yelp_test['label']):\n",
        "    predicted_sentiment = predict_sentiment(text, best_threshold)\n",
        "    pred = 0\n",
        "    if predicted_sentiment == 'positive':\n",
        "      pred = 1\n",
        "    else:\n",
        "      pred = -1\n",
        "    if pred == label:\n",
        "        test_correct_predictions += 1\n",
        "\n",
        "test_accuracy = test_correct_predictions / test_total_predictions\n",
        "print(f\"Test Accuracy with best threshold: {test_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX-tqRXkl20A"
      },
      "source": [
        "Now, please implement a Bag of Words model.  Check if feature selection works and validate on the validation dataset.\n",
        "\n",
        "Which words are most strongly correlated to positive sentiment?  Which are strongly correlated to negative sentiment?  One way to check is find those words with high PMI to positive words (e.g. excellent, great) and those with negative words (bad, terrible).\n",
        "\n",
        "Please work similarly to what we did in the first homework and feel free to adapt your PMI corde from there.\n",
        "\n",
        "Also, similar to the first homework, using the train and test datasets, find the number of features to choose.  Then validate this amount using the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "b19eeU2HmAE_",
        "outputId": "8a9a7a59-6f12-4edd-c50c-57c0738738d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.8718\n",
            "Top 5 Positive Words: ['deliciously' 'divine' 'biz_photos' 'halo' 'downside' 'pleasantly'\n",
            " 'wonderfully' 'exceeded' 'ngreat' 'gem']\n",
            "Top 5 Negative Words: ['ow' 'unprofessional' 'inedible' 'rudest' 'disgusted' 'worst' 'rudely'\n",
            " 'disgusting' 'pathetic' 'questioned']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "X_train = yelp_train['text']\n",
        "y_train = yelp_train['label']\n",
        "X_validate = yelp_validate['text']\n",
        "y_validate = yelp_validate['label']\n",
        "\n",
        "# Vectorize the text using Bag of Words model\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "X_validate_bow = vectorizer.transform(X_validate)\n",
        "\n",
        "# Feature selection (select top-N features)\n",
        "# You can adjust the value of k as needed\n",
        "k = 1000  # Number of top features to select\n",
        "selector = SelectKBest(chi2, k=k)\n",
        "X_train_selected = selector.fit_transform(X_train_bow, y_train)\n",
        "X_validate_selected = selector.transform(X_validate_bow)\n",
        "\n",
        "# Train a classifier (Multinomial Naive Bayes)\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train_selected, y_train)\n",
        "\n",
        "# Evaluate the classifier on the validation set\n",
        "y_pred = clf.predict(X_validate_selected)\n",
        "accuracy = accuracy_score(y_validate, y_pred)\n",
        "print(\"Validation Accuracy:\", accuracy)\n",
        "\n",
        "# Get the top-N features (words) most strongly correlated to positive and negative sentiment\n",
        "feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "selected_feature_names = feature_names[selector.get_support()]\n",
        "\n",
        "# Get the coefficients (log probabilities) of the features\n",
        "coefficients = clf.feature_log_prob_\n",
        "\n",
        "# Sort the coefficients and feature names by coefficient values\n",
        "sorted_indices = np.argsort(coefficients[1] - coefficients[0])  # for binary classification, we only have two classes\n",
        "\n",
        "# Top N positive features\n",
        "top_positive_indices = sorted_indices[-10:][::-1]\n",
        "top_positive_words = selected_feature_names[top_positive_indices]\n",
        "\n",
        "# Top N negative features\n",
        "top_negative_indices = sorted_indices[:10]\n",
        "top_negative_words = selected_feature_names[top_negative_indices]\n",
        "\n",
        "print(\"Top 5 Positive Words:\", top_positive_words)\n",
        "print(\"Top 5 Negative Words:\", top_negative_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szshkQa5mLOg"
      },
      "source": [
        "Now, please implement an embedded model with a CNN similar to the previous homework. Feel free to use either Glove *or* Word2Vec -- whichever worked better for you in the previous homework. No need to check both."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "G2WYz1-vmRHS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "import gensim.downloader as api\n",
        "from keras.layers import TextVectorization\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "glove_model = api.load(\"glove-wiki-gigaword-300\")\n",
        "embedding_dim = glove_model.vector_size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8GDgnrWi8-i-"
      },
      "outputs": [],
      "source": [
        "X_train = yelp_train['text']\n",
        "y_train = yelp_train['label']\n",
        "X_test = yelp_validate['text']\n",
        "y_test = yelp_validate['label']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdr732xq9uMk",
        "outputId": "6cc1ded0-c0a1-488d-ae29-5c1e97432935"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qvtqfRx28oES"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "# Define a function to remove stopwords\n",
        "def remove_stopwords(text_list):\n",
        "    filtered_text_list = []\n",
        "    for text in text_list:\n",
        "        # Convert to string if the element is not already a string\n",
        "        if not isinstance(text, str):\n",
        "            text = str(text)\n",
        "        words = text.split()\n",
        "        filtered_words = [word for word in words if word.lower() not in ENGLISH_STOP_WORDS]\n",
        "        filtered_text = ' '.join(filtered_words)\n",
        "        filtered_text_list.append(filtered_text)\n",
        "    return filtered_text_list\n",
        "\n",
        "# Apply the remove_stopwords function to the list of texts\n",
        "X_train = remove_stopwords(X_train)\n",
        "y_train = remove_stopwords(y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "43Nf5gIi8dpL"
      },
      "outputs": [],
      "source": [
        "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(X_train).batch(128)\n",
        "vectorizer.adapt(text_ds)\n",
        "\n",
        "X_tra = vectorizer(np.array([[s] for s in X_train])).numpy()\n",
        "X_te = vectorizer(np.array([[s] for s in X_test])).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fcl8TC5xMHhT"
      },
      "outputs": [],
      "source": [
        "voc = vectorizer.get_vocabulary()\n",
        "word_index = dict(zip(voc, range(len(voc))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "18ZCFu1Y_If_",
        "outputId": "720cf7a0-6068-4e0f-9409-8137e94b1ae3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converted 17588 words (2412 misses)\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(word_index)\n",
        "num_tokens = len(voc) + 2\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "for word, i in word_index.items():\n",
        "    if word in glove_model:\n",
        "      hits += 1\n",
        "      embedding_matrix[i] = glove_model[word]\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TngV3V2X8idn"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Embedding\n",
        "import keras\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LUeZQzMdApDP"
      },
      "outputs": [],
      "source": [
        "from keras import layers\n",
        "\n",
        "def createModel():\n",
        "  int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "  embedded_sequences = embedding_layer(int_sequences_input)\n",
        "  x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
        "  x = layers.MaxPooling1D(5)(x)\n",
        "  x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "  x = layers.MaxPooling1D(5)(x)\n",
        "  x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "  x = layers.GlobalMaxPooling1D()(x)\n",
        "  x = layers.Dense(128, activation=\"relu\")(x)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  preds = layers.Dense(2, activation=\"sigmoid\")(x)\n",
        "  model = keras.Model(int_sequences_input, preds)\n",
        "  model.summary()\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iDF_Q0zlP8f8"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the labels\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GSmO8pIoArLC",
        "outputId": "d72a852b-ef00-4810-f936-b31bbcb7871e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 300)         6000600   \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, None, 128)         192128    \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1  (None, None, 128)         0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, None, 128)         82048     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPoolin  (None, None, 128)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, None, 128)         82048     \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 128)               0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6373594 (24.31 MB)\n",
            "Trainable params: 372994 (1.42 MB)\n",
            "Non-trainable params: 6000600 (22.89 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "196/196 [==============================] - 65s 326ms/step - loss: 0.4265 - acc: 0.7958 - val_loss: 0.3280 - val_acc: 0.8602\n",
            "Epoch 2/10\n",
            "196/196 [==============================] - 70s 358ms/step - loss: 0.2639 - acc: 0.8936 - val_loss: 0.3133 - val_acc: 0.8628\n",
            "Epoch 3/10\n",
            "196/196 [==============================] - 65s 331ms/step - loss: 0.1734 - acc: 0.9356 - val_loss: 0.3085 - val_acc: 0.8634\n",
            "Epoch 4/10\n",
            "196/196 [==============================] - 64s 329ms/step - loss: 0.0935 - acc: 0.9658 - val_loss: 0.3380 - val_acc: 0.8610\n",
            "Epoch 5/10\n",
            "196/196 [==============================] - 64s 326ms/step - loss: 0.0737 - acc: 0.9734 - val_loss: 0.3535 - val_acc: 0.8656\n",
            "Epoch 6/10\n",
            "196/196 [==============================] - 67s 340ms/step - loss: 0.0397 - acc: 0.9858 - val_loss: 0.4911 - val_acc: 0.8452\n",
            "Epoch 7/10\n",
            "196/196 [==============================] - 65s 332ms/step - loss: 0.0413 - acc: 0.9851 - val_loss: 0.4165 - val_acc: 0.8502\n",
            "Epoch 8/10\n",
            "196/196 [==============================] - 65s 331ms/step - loss: 0.0228 - acc: 0.9925 - val_loss: 0.4824 - val_acc: 0.8582\n",
            "Epoch 9/10\n",
            "196/196 [==============================] - 64s 328ms/step - loss: 0.0283 - acc: 0.9898 - val_loss: 0.5427 - val_acc: 0.8672\n",
            "Epoch 10/10\n",
            "196/196 [==============================] - 65s 332ms/step - loss: 0.0186 - acc: 0.9937 - val_loss: 0.5717 - val_acc: 0.8590\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7de1386766b0>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = createModel()\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"],\n",
        ")\n",
        "type(y_train)\n",
        "model.fit(X_tra, y_train_encoded, batch_size=128, epochs=10, validation_data=(X_te, y_test_encoded))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uy6RMqsmSN1"
      },
      "source": [
        "Next, please try a LSTM model with Keras' word embedding.\n",
        "I personally liked the tutorial here:\n",
        "\n",
        "https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/.  Note that certain lines like:\n",
        "\n",
        "top_words = 5000\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data\n",
        "\n",
        "(num_words=top_words)\n",
        "\n",
        "will need to be tweaked. Feel free to use the number of top_words you had in the Bag-of_words model for the parameter top_words."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM:** https://colab.research.google.com/drive/1AZ7YpNaknv9AI0Z_nJHzw4Z2qIDxJkGC?usp=sharing"
      ],
      "metadata": {
        "id": "r8MiJE0UP3Hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LSTM: https://colab.research.google.com/drive/1AZ7YpNaknv9AI0Z_nJHzw4Z2qIDxJkGC?usp=sharing"
      ],
      "metadata": {
        "id": "1yAScFZrQAlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1u7klOLmBYZ"
      },
      "source": [
        "Now try a transformer model.  While you can train it from scratch, I suggest you don't and use something what we discussed in class:\n",
        "https://colab.research.google.com/drive/15fisDt6RHTdFnkskokD9-jJ9luEbv-z3?usp=sharing\n",
        "\n",
        "While this dataset was developed for SST2 (Stanford Sentiment Treebank v2), feel free to use it \"as is\" and without any fine tuning to the model. However, do please check if a different sentiment threshold would work better for this specific datset similar to what you did in the Vader model. It may be that the threshold will need to be tuned here too."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformer: https://colab.research.google.com/drive/1IZ0OHp8QCKQg5Ua1e6SyGtEu8r-tByTX?usp=sharing**"
      ],
      "metadata": {
        "id": "gbuKdKNIfavP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "L2hHycl7mUaP"
      },
      "outputs": [],
      "source": [
        "Transformer: https://colab.research.google.com/drive/1IZ0OHp8QCKQg5Ua1e6SyGtEu8r-tByTX?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OUK4VqvLsx9"
      },
      "source": [
        "Make sure to reflect about these models and the differences in their performance."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}